---
layout: post
title:  "졸프 관련 논문 정리"
subtitle:   "서브타이틀임"
categories: review

comments: true
---

졸프 관련 논문 정리

**논문1. Deepfake video detection using RNN**

이 논문은 자동으로 deepfake video를 탐지하는 pipeline에 대해 제시

CNN을 통한 frame-level 특징을 얻고 이 특징으로 RNN에 넣어 해당 영상이
조작 됐는지 여부를

판단한다.

Deepfake video를 만드는 방법?

1\. Autoencoder를 통해 차원축소(특징벡터 압축)를 통해 이미지의 loss를
최소화 시키면서 압축된

특징을 얻어낸다.

2\. 가중치를 공유하는 encoder-decoder 세팅을 통해 학습한다.

![](media/image1.png){width="5.427083333333333in"
height="4.760380577427822in"}

Decoder가 아니라 pixel level로 정보를 바꿔야 얼굴이 바뀌는게 아닌가?

Deepfake training

A는 original에 대한 이미지, B는 합성으로 보여주고 싶은 이미지

A,B의 autoencoder는 가중치를 공유해야 한다. 만약 하지 않는다면, 각각의
디코더로 decoding을 할 때 각 이미지에 대한 특징만 얻기 때문에 두
이미지를 합성할 수 없음

디코더는 서로 다른 것을 사용하지만, encoder는 같은 것을 사용했으므로
data로 쓰인 얼굴들의 특성을 알고있어 (눈,코 위치 등..) 자연스러운 합성이
가능

**RNN for deepfake detection**

1\. CNN for frame feature extraction \>\> LSTM에 입력으로 넣을 feature
생성

2\. LSTM for temporal sequence analysis \>\> short frame에 대한 sequence
descriptor 생성

![](media/image2.png){width="6.854665354330709in" height="2.28125in"}

Detection system

Parameter

Training 75 : validation 15 : test 15 비율

매 frame은 299x299로 설정, 각 channel에서 channel mean값을 빼줌

Lr : 1e-5, decay : 1e-6, optimizer : adam, 10 epoch

Subsequence of length N : 20,40,80 frame으로 설정 (24frame 당 1초 정도)

2초정도면 해당 영상이 조작됐는지 여부를 판단 가능

RNN의 장점 : 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는
네트워크 구조

\>\> 필요에 따라 다양하고 유연하게 구조를 만들 수 있다.

근데 RNN의 주용도는 이미지캡션(설명), 감성분석기, 다음단어 예측에
쓰이는데 deepfake와

관련이 있나 ? \>\> 마지막 결과로 fake or real로 판단하는걸 보니 이미지
캡션의 용도로 쓴 듯

**논문2. Deep fakes using GAN**

GAN은 paired setting이 필요 (label)하기 때문에, unsupervised로도
해결하기 위해서

Cycle-GAN을 도입하여 unpaired image translation으로 해결

![](media/image3.png){width="6.8125in" height="3.03125in"}

Cycle-GAN은 2개의 DCGAN으로 구성 (2개의 generator, discriminator)

학습때는 다른 2개의 domain으로부터 다양한 이미지를 2개의 DCGAN input으로
넣는다.

(Domain은 X,Y로 명시) X domain에 포함된 data x를 DCGAN에 넣은 결과는 Y
domain에 가깝게

만들어지며 (Y\^), 반대로 Y domain data y는 다른 DCGAN에 의해 X에 가깝게
만들어짐 (X\^)

(F,G는 generator, Dx,Dy는 discriminator)

**Deepfake 유튜브 정리 (해당 주제를 하게 된 계기제시, + 좀 더 구체적
사례 찾기(그 알) )**

Face manipulation (얼굴을 조작하는 모든 방법들) : face synthesis,
attributes, expression, swap

Deepfake는 face swap(기존에 있던 얼굴 2개를 바꿈)에 해당 함 (deep
learning-based 기술)

Deepfake는 또 임의로 "Few-shot"과 "Multi-shot"으로 구분 됨

Few-shot : generally based-on DL-based face reconstruction (1장\~적은
개수로만 face-swap)

Multi-shot : generally based-on Auto-Encoder (ex : deepfakes, deepface
lab) 여러 장인 경우

Deepfake 악용 사례 : fake news, 사기(사칭), privacy 문제들...

좋게 사용예시 : 스턴트맨에 배우 얼굴 합성

Deepfake의 3단계 : extraction(target의 랜드마크 위치,정보 학습) --
training(AE 기반) -- converting

![](media/image4.png){width="5.368055555555555in"
height="2.4458333333333333in"}

각자의 auto encoder를 통해 결과로 각자의 이미지가 그대로 나오게 만드는데
encoder의 정보를

공유하고 decoder는 각각 사용 encoder는 두 이미지에서의 랜드마크
정보(위치)들을 동시에 학습 보통 similar feature(눈,코,입 위치),
decoder는 각 이미지의 identity feature(생김새,모양)를 학습

![](media/image5.png){width="5.034722222222222in"
height="1.7173611111111111in"}

참조 할 이미지 A로 encoder를 통해 A의 랜드마크 위치들을 파악하고
target(victim)이 될 이미지B의 decoder를 통해 A의 랜드마크에 B의 identity
feature로 변환하여 image swap

Deepfake 할 때 필요한 정보

기본적으로 1000\~10000장의 이미지가 있어야 괜찮은 결과 도출

가능한 여러가지 각도,표정,조명 조건들을 고려하여 dataset에 추가

Nvidia GTX 1080으로(내 컴퓨터 사양) 12\~48시간의 학습시간 소요, 코랩으로
하면 하루정도 걸림

Original model : 64px input,output

Deepfake detection

![](media/image6.png){width="5.159722222222222in"
height="1.8541666666666667in"}

한 장의 이미지가 아니라 video에서 detection을 사용함

합성에 의해 불완전한 feature vector를 감지 \>\> Temporal features across
frames

Faceforensics++ : ICCV 2019 논문 읽어보기 (Deep Classifier에 관해)

Recurrent Convolutional Strategies for Face Manipulation CVPRW 2019 논문
(T F a F에 관해)

FACEforensics library (매 프레임마다 detection을 잘 하도록 설정된
라이브러리)

![](media/image7.png){width="6.293541119860017in"
height="2.638888888888889in"}

![](media/image8.png){width="5.819444444444445in"
height="2.6055555555555556in"}

Deepfake detection을 하려면 faceforensic을 base로 삼아서 해야 함
(XceptionNet을 사용해야 함)

**FaceForensics++: Learning to Detect Manipulated Facial Images ICCV
2019**

(deepfake detection을 automated benchmark로 하는 표준화된 library)

Detection의 첫 과정으로 comporession, resizing을 통해 조작된 pixel을
찾을 수 있음

최종결과로 이상이 있는 pixel에 masking을하여 조작 되었는지의 여부를
알려줌(landmarking)

Deepfake detection model이 상당히 많은 dataset을 요구하는데 src free
DA로 해결이 가능할까?

Pixel의 부자연스러움으로 판단한다면 굳이 많은 dataset이 필요할까??

Src free까진 아니더라도 적은 양의 dataset만으로도 detection을 가능케하는
향상효과 찾아보자

Deepfake detection은 인간이 쉽게 발견할 수 있는 전체적인 얼굴이 바뀌는
것을 감지할 뿐만 아니라 인간이 감지하기 힘든 일부의 특정부위(입 등..)도
감지하기 때문에 유용하다.

![](media/image9.png){width="4.534722222222222in"
height="2.026388888888889in"}

Deepfake detection pipleline

여러가지 neural network 성능(classification 결과 정확도) 비교

![](media/image10.png){width="5.9375in" height="3.6486111111111112in"}

XceptionNet이 우수한 이유 : 이미 ImageNet으로 pretraining + 방대한
capacity

Full Image는 얼굴뿐만이 아닌 사람 전체라 성능이 저하, pristine은 변조x
원본 영상

Benchmark

기존 비디오(raw)를 모호하게 만듦(by re-sizing, compression, bit-rate) :
raw \>\> HQ, LQ

raw일때가 성능이 좋지만 실제 deepfake사례들은 SNS,인터넷에 올려지므로
HQ,LQ상태가 많음

부자연스러운 픽셀을 통해 판단을 하기 때문에, 우선 픽셀을 추출하여 틀만
갖추고 이후에 픽셀레벨을 주변 픽셀들과 비교하여 직접적으로 보정할 수
있나? (interpolation을 통해..)

또 어떻게 하면 full shot에서도 좋은 성능을 낼 수 있을까?

![](media/image11.png){width="5.048611111111111in"
height="1.7215277777777778in"}

Benchmark는 오로지 변조에 대한 detection을 높이는게 목적이므로 변조 없는
real(pristine)에서

정확도가 낮지만 detection에 관하여 성능이 좋기 때문에 real은 신경 쓰지
않는다.

Deepfake는 주로 사람에게 사용하기 때문에 사람에 대한 많은 dataset이
필요한데,

이렇게 되면 data privacy 침범, 또 detection에 많은 dataset이 필요
(무겁다.)

두가지를 한번에 해결하기 위한 적은 dataset만으로도 해결할 수 있는 방도가
있을까?

무작정 dataset을 줄이면 그만큼 학습효과가 적어지기 때문에 학습효과를
유지하며 여러가지

조건에서의 사진들을 어떻게? (GAN ?) + AI ethics

Recurrent Convolutional Strategies for Face Manipulation Detection in
Videos

Recurrent model을 통해 temporal 정보를 추출하여 조작여부를 증명하는
모델을 설명

기존 faceforensic++ 보다 4.55%의 더 높은 정확도를 보임

face조작이 시간적 일관성을 지키진 않으며, frame by frame으로 수행하지도
않기 때문에

이러한 점을 이용하여 detect하는 모델 생성 (by recurrent conv model)

이 모델로 시간의 흐름에 따른 discrepancy를 찾아 낸다.

\+ face alignment method로 이상한 얼굴의 움직임을 잡아낸다.
(부자연스러운 픽셀 검출?)

Video processing with deep model을 통해 temporal information을 얻음
(3가지 방법)

1\. Two stream network : 행동인식엔 효과적이나, 만들어낸 정보는 잘
잡아내지 못함

2\. single stream network supported by recurrent conv layer

: 각 frame img를 CNN에 넣어 특징을 추출(=정보를 얻음) 이 정보를
recurrenct model에 넣어 temporal에 대한 정보를 학습

3\. 3D conv : 만들어낸 정보는 잘 잡아내나, 그 만큼 많은양의 filter가
필요(넘 무겁다.)

Two stream과 3D conv는 다음과 같은 문제로 video의 변조 여부를 확인하는
모델로 2번을 채택

Recurrent model의 작동방식 : 1단계는 cropping & face alignment, 2단계는
manipulation detection

![](media/image12.png){width="6.770833333333333in"
height="3.7083333333333335in"}

Face alignment : removing confounding factor를 하기 위한 수단으로 사용함

1단계

Face alignment을 통해 face recognition을 효율적으로 실행, alignment에는
2가지 방법이 존재

1\) using facial landmarks : 좌표계와 face crop을 통해 face alignment를
실행하고 align이 되어서 얼굴에서 rigid motion(불안정한 움직임?)을
검출한다.

얼굴의 가장 주된 특징들 (눈,코,입 등의 corners) 7곳을 landmarking하여
유사도를 따짐

2\) Spatial Transformer Network (STN)을 통한 implicit alignment based on
affine transformation :

Network가 alignment 아핀 parameter를 예측하고, 얼굴의 특정 부분을 확대한
영상을 학습하여 loss를 최소화 시킨다.

STN은 localization net, grid generator, sampler 3개로 구성되어 있으며,

Localization net은 아핀 변환 파라미터 값을 예측하는 역할

Grid generator와 sampler는 특징맵의 결과를 얻기 위해 아핀 파라미터값을
특징맵의 입력으로

"warping" 시켜주는 역할을 한다. 이를 통해 얼굴의 중요한 지역을
집중적으로 alignment 시킴

위 2개의 alignment의 핵심은 recurrent conv model이 필요하다는 것이다.

Spatio-temporal, tightly aligned face crops의 sequence인 "tubelet"을
입력으로 만들기 위해서

2단계 detection

2단계에서의 input은 video의 sequence of frame

detection에서의 핵심은 frame이 넘어가며 생기는 temporal discrepancy를
찾는 것이다.

Low level를 조작했을 경우에는 temporal하게 보았을 때, 프레임 같은 위치의
특징이 불일치함을 쉽게 파악할 수 있다.

Encoding network

Alignment와 BiDir recurrent를 거치고나서 encoding은 ResNet or DenseNet을
이용

어떠한 Network를 쓰던지, 해당 backbone network는 우선 FF++로 학습되어야
한다.

FF++가 현재까지는 detection을 하기 위한 기반으로 쓰이는 network이기
때문에..

FF++로 학습된 backbone network는 RNN(temporal 정보를 얻기위함)으로
확장되고 결국엔

End-to-end 의 방식으로 학습이 시작된다.

실험결과

1.DenseNet은 ResNet의 성능을 더 증폭시키기 위해 사용

2.face alignment는 실제로도 성능향상에 도움을 줌

3\. 1개의 frame을 줄 때보다 여러 frame을 입력으로 줄 때가 (temporal
information이 있을 때) 더

좋은 성능을 보였다.

4\. 한 방향보다 양 방향의 RNN이 더 좋은 성능을 보임

FaceForensic++ 코드 분석

코드실행도 해보려 했으나 dataset이 최소 250G를 초과하여 실험은 하지 못함

**[[FaceForensics]{.ul}](https://github.com/ondyari/FaceForensics)/[[classification]{.ul}](https://github.com/ondyari/FaceForensics/tree/master/classification)/[[network]{.ul}](https://github.com/ondyari/FaceForensics/tree/master/classification/network)/models.py**

![](media/image13.png){width="4.947916666666667in"
height="3.3333333333333335in"}

Pretrain의 여부를 판단하여 pretrain 되지 않았다면 xceptionNet에 넣어
pretrain을 시켜 True로 만들고, Pretrain이 되었다면 가장 마지막으로
최적화된 last_linear를 fully connected하고

State_dict를 통해 가중치 조절(?)과, xception이 저장된 경로를 load하여
XceptionNet을 적용하여 model.fc를 최적화 시키고 그 값을 다시 model의
last_linear에 넣어 model값을 업데이트한다.

Class TransferModel

![](media/image14.png){width="5.341550743657043in"
height="7.229166666666667in"}

기존 model에서 feature extract 과정을 거치고 최적화된 parameter로
transfer learning을 하는

Transfer Model의 동작과정을 보여주는 코드이다. 먼저 어떤 모델로 학습을
할 것인지 선택한다.

코드상으론 XceptionNet과 ResNet이 있으나, 논문은 XceptionNet만 다루므로
이 부분만 보겠다.

XceptionNet으로 model choice를 하고 학습효과를 높이기 위해 dropout을
시켜준다.

![](media/image15.png){width="5.9375in" height="7.186897419072616in"}

Gradient를 True/False를 통해 Network를 동결 시킬 수 있나? (parameter
업데이트 중지)

\#Make fc trainable에 gradient=True로 둔 것으로 보아 True면 parameter가
업데이트 가능하고

False로 하면 train이 안되므로 Network가 동결됐다고 추측할 수 있다.

근데 여기부분은 바로 이해가 안감 전체적인 코드를 보고 이해해야 할 듯

이렇게 ExceptionNet 내용들을 적용시키기 위해 forward함수를 통해
실행시킨다.

여기까지가 class TransferModel의 내용이다.

![](media/image16.png){width="4.895833333333333in"
height="3.3333333333333335in"}

Class TransferModel을 부르기 전에 어떤 모델로 transfer learning을 할
것인지 정하는 함수이다.

선택사항으로 XceptionNet과 resNet이 있으며, 해당 modelname으로
TransferModel class를

초기화 시켜 실행한다.

[FaceForensics](https://github.com/ondyari/FaceForensics)/[classification](https://github.com/ondyari/FaceForensics/tree/master/classification)/[network](https://github.com/ondyari/FaceForensics/tree/master/classification/network)/**xception.py**
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

![](media/image17.png){width="6.84375in" height="2.28125in"}

단순히 Conv를 적용하는 class

![](media/image18.png){width="6.510416666666667in"
height="6.333035870516185in"}

![](media/image19.png){width="1.8854166666666667in"
height="1.8681988188976377in"}

rep라는 빈 배열에 self.relu를 통해 model에 relu를 적용한 파라미터 값이
들어가고

이후 skip(=Conv2d 적용), skipbn(=batch normalize 2d 적용) 을 거쳐 갱신된
파라미터 값을

가져오는 역할을 하는 class Block이다.

![](media/image20.png){width="5.447916666666667in"
height="9.333333333333334in"}

먼저 block을 12번 호출하여 층을 쌓아가는 과정(grow)을 거치고 마지막
12번째 block에선

Grow_first를 false로 두어 쌓는 과정을 멈춘다. 이후 별도의 conv, batch
normalize를 거쳐

최종적으로 2048차원의 vector를 만들고 이것을 fc를 통해 output을 class로
보여준다.

이 과정은 여느 feature extractor와 비슷한 역할을 하는 Class Xception
이었다.

![](media/image21.png){width="5.75in" height="3.6649398512685916in"}

**XceptionNet** **: Deep learning with depthwise separable
convolutions**

코드만 봐서는 정확히 돌아가는 model이 이해가 가질 않아 XceptionNet에
관해 다룬 블로그를 참조 (출처 :
https://sike6054.github.io/blog/paper/fifth-post/\#11-the-inception-hypothesis)

이 논문에서는 Inception module을 일반적인 conv와 depthwise separable
conv의 중간단계로

해석하는 방법을 제시한다.

Depthwise separable conv는 depthwise conv 뒤에 pointwise conv가 뒤따르는
형태이다.

이러한 관점에서 Depthwise separable conv는 가장 많은 수의 conv, pooling
layer가 있는 inception module이라고 볼 수 있다.

확장된 inception module을 기반으로 새로운 CNN구조가 제시되었으며, 이것이
XceptionNet이다.

CNN에서 feature extraction은 conv연산이, spatial sub-sampling은
max-pooling연산을 통해

수행하며, max-pooling 사이에 여러 번의 conv를 넣어 네트워크가 모든
spatial(wide-height) scale에 대해 풍부한 feature를 학습하게 된다.

(원본 이미지에서 점점 줌인해 가며(by pooling 다양한 spatial 정보)
feature 획득(conv를 통해))

XceptionNet은 inception-V3에서 파생된 모델이며 V3에 비해 더 많은 data,
class 에서

좋은 성능을 보여준다.

![](media/image22.png){width="6.055555555555555in"
height="2.7666666666666666in"}

Inception-V3 module 여기서 input은 입력으로 들어오는 특징맵이며

1x1 conv를 적용해서 차원을 줄여, 마지막에 concat할 때 channel개수가 너무
커지던 문제를

해결하고, 3x3 conv에서도 1x1 conv를 통해 채널 작게만들어, input 특징맵과
conv연산을 할 때

Parameter 수를 줄여 연산량과 메모리측면에서 이득을 챙길 수 있다.

그리하여 inception은 적은 parameter로 풍부한 표현을 학습할 수 있다.

더 정확히 설명하면, 기본 conv연산은 공간차원(width,height)과 channel
차원으로 이루어진 3D 공간에 대한 filter를 학습한다. 즉, 1개의 conv연산은
channel과 spatial을 동시에 매핑하는 작업을 수행한다.

Inception의 철학은 channel, spatial을 동시에 보지 말고 독립적으로 보도록
분리시켜서 이 과정을

쉽고 효율적으로 만들게 해준다.

이 분리하는 과정을 먼저 1x1 conv를 통해 channel correlation을 보고,
input보다 채널이 작은 3\~4개의 분리된 공간에 mapping한다. 그 다음 보다
작아진 3D 공간에 3x3 or 5x5 conv를 수행하여 spatial correlation을
mapping한다. 이렇게 2가지를 동시에 매핑하는 것을 방지한다.

![](media/image23.png){width="5.388888888888889in"
height="2.027083333333333in"}

Inception module의 extreme version

1x1 conv로 channel correlation을 mapping하고, output channel의 spatial
correlation을

각 channel에 대해 mapping한다.

Extreme form이 depthwise separable conv와 거의 동일하다.

depthwise separable conv는 input의 각 channel에 대해 독립적으로 spatial
conv가 수행되고,

그 뒤로는 pointwise conv(1x1 conv)가 이어지는 형태이다.

Pointwise conv는 depthwise conv결과인 output channel을 새로운 channel
space(segment)에

projection하는 작업이다.

그럼 inception module의 Extreme ver과 separable conv의 차이는 ??

1\. operation 순서

\- inception은 1x1 conv를 먼저 수행하지만, separable conv는 spatial
conv를 먼저 수행하고

1x1 conv를 수행한다.

2\. operation뒤의 non-linearity 여부

\- inception에서는 두 operation 모두 비선형 활성함수로 ReLU가 따르지만,
separable conv는

일반적으로 비선형 활성함수 없이 구현된다.

Depthwise separable conv는 input channel의 수에 따라 parameter 개수가
결정된다.(N개=채널수)

일반적인 conv는 1x1 conv가 선행되면 전체 channel을 하나의 segment로
취급(1개)

Inception module은 수 백개의 채널을 3\~4개의 segment로 다룸 (depth와
일반 conv의 사이)

XceptionNet 구조 (전적으로 depthwise separable conv에 기반한 CNN구조)

Extreme Inception module의 구조를 따르기 때문에 둘을 합쳐 Xception이라
불린다.

![](media/image24.png){width="5.978595800524935in" height="4.0625in"}

Xception 구조 : Entry-middle(8번반복)-exit flow를 거친다. 모든 conv
이후엔 BN을 함

요약하면 Xception 구조는 residual connection이 있는 depthwise separable
conv의

Linear stack으로 볼 수 있다.

또, Xception 구조에서 residual connection의 여부에 따른 정확도와
step차이에서 명확한 차이가 발생했다. \>\> xception 구조를 사용할거면
residual connection은 필수조건이다.

**Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics**

기존의 deepfake dataset은 너무 품질이 좋지않고 여러 문제가 있어서
DeepFake detection의

성능을 높이기 위한 새로운 deepfake dataset을 만들었다는 논문

Grad-CAM: Visual Explanations from Deep Networks via Gradient-based
Localization

CAM은 Class Activation Mapping의 준말로 CNN 기반 네트워크에서 많은
클래스를 결정할 때,

시각적인 설명을 제공합니다. (= CAM은 CNN의 내부를 열어볼 수 있는 아주
유용한 도구입니다.

기존 CNN의 conv layer 뒤에 있는 fc layer를 GAP(Global Average
Pooling)으로 교체하고

Fine Tuning함으로써, 네트워크가 이미지의 어떤 부분을 보고 특정 레이블로
판단을 내리는지

알 수 있기 때문입니다.) inception이나 grad-cam이나 GAP를 활용하므로
GAP을 동시활용가능?

기존은 왜 모델이 잘 작동하는지, 어떻게 서로 다른 class를 분류하는지에
대해 알 수 없었고,

오직 모델이 각 클래스에 출력하는 확률만 알 수 있었는데, CAM을 통해 그
문제를 해소

GAP으로 바꿔도 classification 성능이 떨어지지 않는다는 장점이 있지만,

CAM은 FC layer를 GAP으로 바꿔줘야 한다는 점, GAP직전의 conv layer만 쓸
수 있다는 점,

GAP 뒤 Dense Layer의 가중치가 필요해 fine tuning이나 retraining을 해야
하는 단점이 있다.

(기존의 GAP은 재학습이 필요 없었기 때문이다.

기존의 pooling은 중간중간 해주었지만, GAP은 가장 마지막 layer에만
pooling을 해주어 global함)

이러한 단점 때문에 기본적인 detection외에 VQA, Captioning의 목적을
수행하는 CNN에선

(Captioning : 이미지를 설명하는 문장을 만들어 내는 것)

CAM 적용이 힘들다. (CAM은 only detection 용도) \>\> Grad-CAM의 대두

Grad-CAM : Generalized version of CAM

Grad-CAM은 기존 CAM과 달리 GAP이 아닌 FC에도, 최종 Conv layer가
아니더라도 사용가능 함

여기서 말하는 Grad(Gradient-Weighted)는 CAM을 구할 때, 타겟에 대한
gradient를 이용합니다.

Activation layer에 대해 이해가 잘 안감

(왜 k번째 activation layer부터 class를 잘 잡기 시작하지?) 이건 CAM을
봐야할 듯

<https://jsideas.net/class_activation_map/> 참고해서 보기

Grad-CAM을 통해 얻을 수 있는 이점으로 5가지가 있습니다.

1\) 예측이 실패하면 왜 이것이 실패했는지 설명을 할 수 있습니다.

2\) Adversarial 이미지에 대해 적용이 잘됩니다. (robust)

3\) 기존 방법보다 성능이 좋습니다.

4\) 근본적인 모델에 대해 더 신뢰할만 합니다.

5\) 데이터셋의 bias를 동일시하여 모델 일반화를 시킵니다.

Grad-CAM을 통해 이미지의 어떤 영역이 특정 클래스에 반응하는지 히트맵으로
시각화 할 수

있게 됐지만, 히트맵은 이미지의 특성을 상세하게 보기 어렵다는 점이 있다.
이러한 문제를

Guided Grad-CAM을 통해 해결했다.

Guided Grad-CAM은 영역을 잡아내는 Grad-CAM에 명확한 이미지 윤곽을
반환하는

Guided backpropagation의 장점을 접목한 개념이다.

![](media/image25.png){width="6.854166666666667in" height="3.34375in"}

C,F,I,L에서 빨간 영역은 그 class에 대해 high score를 의미한다.(가장 해당
class같은 영역)

![](media/image26.png){width="7.020833333333333in"
height="4.010416666666667in"}

Grad-CAM의 overview

![](media/image27.png){width="5.611111111111111in"
height="1.2127668416447943in"}

Top Loc error : 특정 몇 개의 위치를 예측했을 때 error일 확률

Top Cls error : 특정 몇 개의 클래스를 예측했을 때 error일 확률

![](media/image28.png){width="5.840277777777778in"
height="2.8870209973753282in"}

위 그림에서 Grad CAM의 예측은 틀렸지만 틀린 이유에 대해 설명(예측 클래스
제시)을 해준다.

![](media/image29.png){width="5.652777777777778in"
height="1.6902777777777778in"}

(a)는 원본 이미지이고 (b)는 Network가 잘못된 예측을 하도록(비행기라고
판단하도록)

(a)의 이미지에서 카테고리를 개, 고양이가 아닌 비행기(Airliner)로
설정하고(Adversarial)

Grad-CAM을 실행했으나, (b)의 의도와 달리 Grad-CAM은 Adversarial
noise에도 robust하게

제대로 된 정답을 찾아내는 것을 확인할 수 있었다.

Counterfactual Explanations

이는 네트워크의 결정을 바꾸는데 영향을 주는 지역을 강조하는 방법이다.

강조한 지역을 배제하고 나머지 지역에서 클래스를 판단한다.

![](media/image30.png){width="3.6718799212598423in"
height="1.4791666666666667in"}

![](media/image31.png){width="4.173611111111111in"
height="1.4509492563429571in"}

![](media/image32.png){width="6.527083333333334in"
height="1.6354166666666667in"}

최종적으로 이미지에 대한 상황을 글로써 서술하면 글에 해당하는
주요단어들을

Grad-CAM을 통해 강조할 수 있다.

CAM : Learning Deep Features for Discriminative Localization (2015)

input image를 넣고 무엇인지 구분하는 classification 작업은 CNN을 통해
해결할 수 있다.

하지만 왜 그 모델이 잘 동작하는지 어떤 기준으로 구분을 하는지에 대해서는
알 수 없고

CNN을 통해 알 수 있는 정보는 각 input에 대한 class의 확률 값만 알 수
있었다.

'왜 잘 동작하는가'에 대한 질문의 답으로 이 논문에서 pretrained network를

약간만 변형해서 놀라운 결과를 내는 Class Activation Mapping (CAM)을
제안한다.

해당 논문을 요약하면

\- \[핵심\] 바운딩 박스를 인풋으로 주지 않아도, 오브젝트 디텍션 용도로
학습된 모델을 조금만

튜닝해서 오브젝트 위치 추적이 가능하다.

\- 탐지 대상인 사물의 위치정보가 없어도 CNN은 오브젝트 디텍터로서
기능하지만, 분류를 위해

Fully connected layer를 사용함으로써 디텍션의 기능이 사라진다.

\- Network In Network나 GoogLeNet에서는 파라미터 수 최소화를 위해 fc
layer대신

GAP(Global Average Pooling)을 쓴다.

\- GAP은 파라미터 수를 줄여 오비피팅을 방지하는 기능 외에도, 오브젝트의
위치정보를

보존하는데 사용할 수 있다.

\- GAP을 통해 특정 클래스에 반응하는 영역을 매핑하는 CAM을 제안한다.

\- 당시 제안된 다른 위치 추적방식들에 비해 CAM은 한번에(single forward
pass) end-to-end로

학습할 수 있다.

\- FC를 GAP으로 대체해도 성능저하가 크게 일어나지 않았으며 이를 보정하기
위한 아키텍처도 제안한다.

\- Global Max Pooling은 탐지 사물을 포인트(점)으로 짚는 반면, GAP은
사물의 위치를 점이 아닌

범위로 잡아내는 장점이 있다.

\- 다른 데이터셋을 활용한 분류, 위치 특정, 컨셉 추출에도 쉽게 대입해서
사용할 수 있다.

FC layer로만 연결된 DNN은 28x28 이미지를 784차원으로 찌그러뜨린 다음, 각
픽셀값에

가중치를 곱해 다음 레이어로 넘긴다. 자연히 이 과정에서 인접 픽셀간의
관계 정보가 손실된다.

이와 달리 CNN은 conv filter를 통해, 인접 픽셀간의 정보를 그대로 유지한채
뒤 레이어에 넘긴다.

이렇게 함으로써 처음엔 직선이나 곡선 정보를 얻고, 이를 조합해서 숫자 2,5
의 형상정보를 얻음

이처럼 어떻게 구분했냐에 대한 이유는 conv filter가 알고 있기 때문에,

Filter가 활성화 되는 위치를 역으로 추적한다면, 그 이미지에서 얼굴이
위치한 부분을

알아낼 수 있다고 생각해볼 수 있다.

하지만 여느 CNN의 마지막 과정처럼 conv filter결과를 fc layer를 통해
flatten 시킨다면

Conv filter에 있던 정보들이 사라지기 때문에 논문에선 fc 대신 GAP를
제안한 것이다.

![](media/image33.png){width="6.020833333333333in"
height="2.0208333333333335in"}

GAP는 위 그림에서 flatten이 일어나기 직전, 마지막 conv layer에 적용하는
방식으로

각 feature map의 평균값을 뽑아 벡터를 만든다.

이해를 위해 마지막 conv layer의 feature map개수가 3개고 각각의 크기는
3x3라고 해보자

![](media/image34.png){width="5.427083333333333in"
height="1.4479166666666667in"}

Average는 각 feature map의 모든 요소들을 합쳤고, Max는 가장 큰 값만
넣었다.

Average라 평균을 취해야 할 것 같지만, 논문에서는 그냥 합만 넣어주었고
나누지 않은 이유는

어짜피 모든 feature map의 크기는 똑같기 때문에 안 나눠주어도 비율은
똑같아서 안 한 것 같다.

논문에선 하지 않았지만, 코드를 살펴보니 return K.mean(inputs, axis\[1,
2\])로

평균을 구하는 것으로 구현을 하였다.

이렇게 GAP을 통해 값을 얻었고 이 값을 통해 CAM을 도출하는 과정을
설명하겠다.

![](media/image35.png){width="0.5486111111111112in"
height="0.243827646544182in"}는 마지막 conv layer의 k번째 유닛의
activation을 표현한다.

즉, 위 그림의 빨간색 표가 f1(x,y)가 된다. K번째 유닛에 대해 GAP을 씌운
값을 ![](media/image36.png){width="0.3645833333333333in"
height="0.2916666666666667in"}라 한다.

같은 예시로 빨간 표의 GAP으로 나온 값 4가
![](media/image37.png){width="0.2152777777777778in"
height="0.2301246719160105in"}이다.

여기서 ![](media/image38.png){width="0.3333333333333333in"
height="0.3333333333333333in"}와
![](media/image39.png){width="0.2604166666666667in"
height="0.3541666666666667in"}를 곱하고 이를 모두 더해 softmax layer에
넣을 input인 Sc를 구한다.

위의 그림을 계속 예로 들자면 Sc를 구하는 과정은 다음과 같이 표현할 수
있다.

![](media/image40.png){width="5.322916666666667in"
height="1.4166666666666667in"}

class 1,2,3 중에서 우리가 관심있는 class가 1이라고 가정하자.

그렇다면 관심있는 softmax input은 S1이 되고, 이를 구하는 공식은
![](media/image41.png){width="0.9375in" height="0.2604166666666667in"}이
된다.

계산 값으로 S1=11이 된다. Sc를 구했으니 이제 Softmax 공식에 넣어 Class
c의 분류 확률을 구함

논문에서는 softmax 직전까지의 공식을 변형하여 CAM을 구한다.

Sc를 구하는 공식은 ![](media/image42.png){width="1.0814512248468942in"
height="0.25534339457567806in"}이며,
![](media/image43.png){width="2.4479166666666665in" height="0.28125in"}

Sc는 Fk대신 위의 식을
대입하여![](media/image44.png){width="2.3333333333333335in"
height="0.2951388888888889in"}로 표현할 수 있고

여기서 우리가 최종적으로 구하고자 하는 Class Activation Map인 Mc(x,y)를
정의 할 수 있다.

![](media/image45.png){width="2.5208333333333335in"
height="0.5833333333333334in"} ,
![](media/image46.png){width="1.7708333333333333in"
height="0.5416666666666666in"}

![](media/image47.png){width="6.268055555555556in"
height="2.520138888888889in"}

실험결과 성능을 평가해보니 VGGnet-GAP의 성능이 다른 VGGnet보다 월등히
좋았고,

fc layer대신 GAP을 쓰다 보니 parameter의 수도 현저히 줄었음을 확인할 수
있다.

하지만 이는 이미지에 대해서만 그렇고, 영상에 CAM을 수행한 결과는
생각보다 이상한 부분들도

얼굴이라 인식하였다. 트레이닝 데이터와 영상 프레임의 이미지 특성이
다르거나

리사이징을 하는 과정에서 정확도가 희생되어 그렇다고 생각이 들고, 아직
정확한 원인은

이미지넷 분류나 segmentation에 관한 연구 결과를 봐야 알 것 같다.

이번주차는 우선적으로 주제 구현을 위한 관련된 큼직한 논문들을 분석해
보았고,

다음 주차부터는 GPU 서버를 통해 기본적인 detection을 제대로 하는지
학습해 보고,

잘 된다면 어떻게 CAM과 연동할 지 연구해볼 예정이다.
